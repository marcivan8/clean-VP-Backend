// utils/emotionAnalyzer.js - Analyse des √©motions avec TensorFlow.js
let tf, faceDetection, faceLandmarkDetectorModel;
let tfAvailable = false;

try {
  tf = require('@tensorflow/tfjs-node');
  faceDetection = require('@tensorflow-models/face-detection');
  faceLandmarkDetectorModel = require('@tensorflow-models/face-landmarks-detection');
  tfAvailable = true;
} catch (e) {
  console.warn('‚ö†Ô∏è TensorFlow dependencies not available. Emotion analysis will be disabled.', e.message);
}
const { createCanvas, loadImage } = require('canvas');
const fs = require('fs');
const path = require('path');

// Note: Les mod√®les TensorFlow seront t√©l√©charg√©s automatiquement au premier usage

let faceDetector = null;
let faceLandmarkDetector = null;

/**
 * Initialise les mod√®les de d√©tection de visages
 */
async function initializeModels() {
  if (!tfAvailable) return;

  try {
    if (!faceDetector) {
      console.log('üîÑ Initialisation du d√©tecteur de visages...');
      const model = faceDetection.SupportedModels.MediaPipeFaceDetector;
      const detectorConfig = {
        runtime: 'tfjs',
        modelType: 'short',
        maxFaces: 10
      };
      faceDetector = await faceDetection.createDetector(model, detectorConfig);
      console.log('‚úÖ D√©tecteur de visages initialis√©');
    }

    if (!faceLandmarkDetector) {
      console.log('üîÑ Initialisation du d√©tecteur de landmarks...');
      faceLandmarkDetector = await faceLandmarkDetectorModel.createDetector(
        faceLandmarkDetectorModel.SupportedModels.MediaPipeFaceMesh,
        {
          runtime: 'tfjs',
          refineLandmarks: true,
          maxFaces: 10
        }
      );
      console.log('‚úÖ D√©tecteur de landmarks initialis√©');
    }
  } catch (error) {
    console.error('‚ùå Erreur initialisation mod√®les:', error);
    // Don't throw, just log and continue without emotion analysis
    tfAvailable = false;
  }
}

/**
 * Analyse les √©motions sur une image
 * @param {string} imagePath - Chemin vers l'image
 * @returns {Promise<Object>} - R√©sultats de l'analyse des √©motions
 */
async function analyzeEmotions(imagePath) {
  try {
    if (!fs.existsSync(imagePath)) {
      throw new Error(`Image non trouv√©e: ${imagePath}`);
    }

    if (!tfAvailable) {
      return {
        facesDetected: 0,
        emotions: [],
        averageEmotion: null,
        dominantEmotion: 'neutral',
        note: 'Emotion analysis disabled due to missing dependencies'
      };
    }

    // Initialiser les mod√®les si n√©cessaire
    await initializeModels();

    // Charger l'image
    const image = await loadImage(imagePath);
    const canvas = createCanvas(image.width, image.height);
    const ctx = canvas.getContext('2d');
    ctx.drawImage(image, 0, 0);

    // D√©tecter les visages
    const faces = await faceDetector.estimateFaces(canvas);

    if (faces.length === 0) {
      return {
        facesDetected: 0,
        emotions: [],
        averageEmotion: null,
        dominantEmotion: null
      };
    }

    // Analyser les landmarks pour chaque visage
    const landmarks = await faceLandmarkDetector.estimateFaces(canvas);

    // Analyser les √©motions bas√©es sur les expressions faciales
    const emotions = faces.map((face, index) => {
      const emotion = analyzeFacialExpression(face, landmarks[index]);
      return {
        faceIndex: index,
        confidence: face.score,
        boundingBox: face.box,
        emotion: emotion
      };
    });

    // Calculer l'√©motion moyenne
    const emotionCounts = {};
    emotions.forEach(e => {
      const emo = e.emotion.dominant;
      emotionCounts[emo] = (emotionCounts[emo] || 0) + 1;
    });

    const dominantEmotion = Object.keys(emotionCounts).reduce((a, b) =>
      emotionCounts[a] > emotionCounts[b] ? a : b
    );

    return {
      facesDetected: faces.length,
      emotions: emotions,
      averageEmotion: calculateAverageEmotion(emotions),
      dominantEmotion: dominantEmotion
    };
  } catch (error) {
    console.error('‚ùå Erreur analyse √©motions:', error);
    return {
      facesDetected: 0,
      emotions: [],
      averageEmotion: null,
      dominantEmotion: null,
      error: error.message
    };
  }
}

/**
 * Analyse l'expression faciale bas√©e sur les landmarks
 * @param {Object} face - D√©tection de visage
 * @param {Object} landmarks - Landmarks faciaux
 * @returns {Object} - √âmotion d√©tect√©e avec scores
 */
function analyzeFacialExpression(face, landmarks) {
  if (!landmarks || !landmarks.keypoints) {
    return {
      dominant: 'neutral',
      scores: { neutral: 0.5, happy: 0, sad: 0, angry: 0, surprised: 0, fearful: 0 }
    };
  }

  const keypoints = landmarks.keypoints;
  const scores = {
    neutral: 0.3,
    happy: 0,
    sad: 0,
    angry: 0,
    surprised: 0,
    fearful: 0
  };

  // Analyser la position des points cl√©s pour d√©terminer l'√©motion
  // Cette logique est simplifi√©e - une vraie impl√©mentation utiliserait un mod√®le ML

  // Points pour les yeux
  const leftEye = keypoints.find(kp => kp.name === 'leftEye');
  const rightEye = keypoints.find(kp => kp.name === 'rightEye');

  // Points pour la bouche
  const mouth = keypoints.find(kp => kp.name === 'mouth');
  const upperLip = keypoints.find(kp => kp.name === 'upperLip');
  const lowerLip = keypoints.find(kp => kp.name === 'lowerLip');

  // Logique simplifi√©e bas√©e sur les positions
  if (mouth && upperLip && lowerLip) {
    const mouthOpen = Math.abs(upperLip.y - lowerLip.y);
    if (mouthOpen > 10) {
      scores.surprised += 0.4;
    }
  }

  // D√©terminer l'√©motion dominante
  const dominant = Object.keys(scores).reduce((a, b) =>
    scores[a] > scores[b] ? a : b
  );

  return {
    dominant,
    scores
  };
}

/**
 * Calcule l'√©motion moyenne sur plusieurs visages
 * @param {Array} emotions - Tableau d'√©motions
 * @returns {Object} - √âmotion moyenne
 */
function calculateAverageEmotion(emotions) {
  if (emotions.length === 0) return null;

  const avgScores = {
    neutral: 0,
    happy: 0,
    sad: 0,
    angry: 0,
    surprised: 0,
    fearful: 0
  };

  emotions.forEach(e => {
    Object.keys(avgScores).forEach(emotion => {
      avgScores[emotion] += e.emotion.scores[emotion] || 0;
    });
  });

  Object.keys(avgScores).forEach(emotion => {
    avgScores[emotion] /= emotions.length;
  });

  const dominant = Object.keys(avgScores).reduce((a, b) =>
    avgScores[a] > avgScores[b] ? a : b
  );

  return {
    dominant,
    scores: avgScores
  };
}

/**
 * Analyse les √©motions sur plusieurs frames
 * @param {string[]} framePaths - Chemins vers les frames
 * @returns {Promise<Object>} - R√©sultats agr√©g√©s
 */
async function analyzeEmotionsBatch(framePaths) {
  const results = [];

  for (const framePath of framePaths) {
    try {
      const result = await analyzeEmotions(framePath);
      results.push(result);
    } catch (error) {
      console.error(`Erreur analyse frame ${framePath}:`, error);
    }
  }

  // Agr√©ger les r√©sultats
  const totalFaces = results.reduce((sum, r) => sum + r.facesDetected, 0);
  const allEmotions = results.flatMap(r => r.emotions);

  const emotionCounts = {};
  allEmotions.forEach(e => {
    const emo = e.emotion.dominant;
    emotionCounts[emo] = (emotionCounts[emo] || 0) + 1;
  });

  const overallDominant = Object.keys(emotionCounts).reduce((a, b) =>
    emotionCounts[a] > emotionCounts[b] ? a : b, 'neutral'
  );

  return {
    totalFacesDetected: totalFaces,
    framesAnalyzed: results.length,
    results: results,
    overallDominantEmotion: overallDominant,
    emotionDistribution: emotionCounts
  };
}

module.exports = {
  analyzeEmotions,
  analyzeEmotionsBatch,
  initializeModels
};

